{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:50:44.060008Z",
     "start_time": "2024-10-11T07:50:44.054079Z"
    }
   },
   "source": [
    "import time\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:50:44.069636Z",
     "start_time": "2024-10-11T07:50:44.061917Z"
    }
   },
   "source": [
    "class Env:\n",
    "    def __init__(self, length, height):\n",
    "        # define the height and length of the map\n",
    "        self.length = length\n",
    "        self.height = height\n",
    "        # define the agent's start position\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "    def render(self, frames=50):\n",
    "        for i in range(self.height):\n",
    "            if i == 0:  # cliff is in the line 0\n",
    "                line = ['S'] + ['x'] * (self.length - 2) + ['T']  # 'S':start, 'T':terminal, 'x':the cliff\n",
    "            else:\n",
    "                line = ['.'] * self.length\n",
    "            if self.x == i:\n",
    "                line[self.y] = 'o'  # mark the agent's position as 'o'\n",
    "            print(''.join(line))\n",
    "        print('\\033[' + str(self.height + 1) + 'A')  # printer go back to top-left \n",
    "        time.sleep(1.0 / frames)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"4 legal actions, 0:up, 1:down, 2:left, 3:right\"\"\"\n",
    "        change = [[0, 1], [0, -1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.height - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.length - 1, max(0, self.y + change[action][1]))\n",
    "\n",
    "        states = [self.x, self.y]\n",
    "        reward = -1\n",
    "        terminal = False\n",
    "        if self.x == 0:  # if agent is on the cliff line \"SxxxxxT\"\n",
    "            if self.y > 0:  # if agent is not on the start position \n",
    "                terminal = True\n",
    "                if self.y != self.length - 1:  # if agent falls\n",
    "                    reward = -100\n",
    "        return reward, states, terminal\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:50:44.077078Z",
     "start_time": "2024-10-11T07:50:44.070672Z"
    }
   },
   "source": [
    "class Q_table:\n",
    "    def __init__(self, length, height, actions=4, alpha=0.1, gamma=0.9):\n",
    "        self.table = [0] * actions * length * height  # initialize all Q(s,a) to zero\n",
    "        self.actions = actions\n",
    "        self.length = length\n",
    "        self.height = height\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _index(self, a, x, y):\n",
    "        \"\"\"Return the index of Q([x,y], a) in Q_table.\"\"\"\n",
    "        return a * self.height * self.length + x * self.length + y\n",
    "\n",
    "    def _epsilon(self):\n",
    "        return 0.1\n",
    "        # version for better convergence:\n",
    "        # \"\"\"At the beginning epsilon is 0.2, after 300 episodes decades to 0.05, and eventually go to 0.\"\"\"\n",
    "        # return 20. / (num_episode + 100)\n",
    "\n",
    "    def take_action(self, x, y, num_episode):\n",
    "        \"\"\"epsilon-greedy action selection\"\"\"\n",
    "        if random.random() < self._epsilon():\n",
    "            return int(random.random() * 4)\n",
    "        else:\n",
    "            actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n",
    "            return actions_value.index(max(actions_value))\n",
    "\n",
    "    def max_q(self, x, y):\n",
    "        actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n",
    "        return max(actions_value)\n",
    "\n",
    "    def update(self, a, s0, s1, r, is_terminated):\n",
    "        # both s0, s1 have the form [x,y]\n",
    "        q_predict = self.table[self._index(a, s0[0], s0[1])]\n",
    "        if not is_terminated:\n",
    "            q_target = r + self.gamma * self.max_q(s1[0], s1[1])\n",
    "        else:\n",
    "            q_target = r\n",
    "        self.table[self._index(a, s0[0], s0[1])] += self.alpha * (q_target - q_predict)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:50:44.084126Z",
     "start_time": "2024-10-11T07:50:44.078882Z"
    }
   },
   "source": [
    "def cliff_walk():\n",
    "    env = Env(length=12, height=4)\n",
    "    table = Q_table(length=12, height=4)\n",
    "    for num_episode in range(3000):\n",
    "        # within the whole learning process\n",
    "        episodic_reward = 0\n",
    "        is_terminated = False\n",
    "        s0 = [0, 0]\n",
    "        while not is_terminated:\n",
    "            # within one episode\n",
    "            action = table.take_action(s0[0], s0[1], num_episode)\n",
    "            r, s1, is_terminated = env.step(action)\n",
    "            table.update(action, s0, s1, r, is_terminated)\n",
    "            episodic_reward += r\n",
    "            # env.render(frames=100)\n",
    "            s0 = s1\n",
    "        if num_episode % 20 == 0:\n",
    "            print(\"Episode: {}, Score: {}\".format(num_episode, episodic_reward))\n",
    "        env.reset()"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:50:44.309486Z",
     "start_time": "2024-10-11T07:50:44.085228Z"
    }
   },
   "source": [
    "cliff_walk()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: -100\n",
      "Episode: 20, Score: -123\n",
      "Episode: 40, Score: -206\n",
      "Episode: 60, Score: -111\n",
      "Episode: 80, Score: -102\n",
      "Episode: 100, Score: -123\n",
      "Episode: 120, Score: -33\n",
      "Episode: 140, Score: -46\n",
      "Episode: 160, Score: -23\n",
      "Episode: 180, Score: -23\n",
      "Episode: 200, Score: -17\n",
      "Episode: 220, Score: -28\n",
      "Episode: 240, Score: -13\n",
      "Episode: 260, Score: -22\n",
      "Episode: 280, Score: -19\n",
      "Episode: 300, Score: -27\n",
      "Episode: 320, Score: -102\n",
      "Episode: 340, Score: -13\n",
      "Episode: 360, Score: -20\n",
      "Episode: 380, Score: -17\n",
      "Episode: 400, Score: -15\n",
      "Episode: 420, Score: -13\n",
      "Episode: 440, Score: -13\n",
      "Episode: 460, Score: -14\n",
      "Episode: 480, Score: -15\n",
      "Episode: 500, Score: -13\n",
      "Episode: 520, Score: -18\n",
      "Episode: 540, Score: -103\n",
      "Episode: 560, Score: -13\n",
      "Episode: 580, Score: -111\n",
      "Episode: 600, Score: -15\n",
      "Episode: 620, Score: -15\n",
      "Episode: 640, Score: -20\n",
      "Episode: 660, Score: -14\n",
      "Episode: 680, Score: -13\n",
      "Episode: 700, Score: -13\n",
      "Episode: 720, Score: -13\n",
      "Episode: 740, Score: -18\n",
      "Episode: 760, Score: -25\n",
      "Episode: 780, Score: -15\n",
      "Episode: 800, Score: -19\n",
      "Episode: 820, Score: -106\n",
      "Episode: 840, Score: -102\n",
      "Episode: 860, Score: -106\n",
      "Episode: 880, Score: -17\n",
      "Episode: 900, Score: -107\n",
      "Episode: 920, Score: -21\n",
      "Episode: 940, Score: -13\n",
      "Episode: 960, Score: -13\n",
      "Episode: 980, Score: -109\n",
      "Episode: 1000, Score: -109\n",
      "Episode: 1020, Score: -108\n",
      "Episode: 1040, Score: -17\n",
      "Episode: 1060, Score: -100\n",
      "Episode: 1080, Score: -13\n",
      "Episode: 1100, Score: -13\n",
      "Episode: 1120, Score: -15\n",
      "Episode: 1140, Score: -13\n",
      "Episode: 1160, Score: -13\n",
      "Episode: 1180, Score: -13\n",
      "Episode: 1200, Score: -17\n",
      "Episode: 1220, Score: -110\n",
      "Episode: 1240, Score: -25\n",
      "Episode: 1260, Score: -14\n",
      "Episode: 1280, Score: -13\n",
      "Episode: 1300, Score: -21\n",
      "Episode: 1320, Score: -110\n",
      "Episode: 1340, Score: -13\n",
      "Episode: 1360, Score: -13\n",
      "Episode: 1380, Score: -13\n",
      "Episode: 1400, Score: -13\n",
      "Episode: 1420, Score: -13\n",
      "Episode: 1440, Score: -15\n",
      "Episode: 1460, Score: -15\n",
      "Episode: 1480, Score: -13\n",
      "Episode: 1500, Score: -13\n",
      "Episode: 1520, Score: -13\n",
      "Episode: 1540, Score: -104\n",
      "Episode: 1560, Score: -15\n",
      "Episode: 1580, Score: -13\n",
      "Episode: 1600, Score: -13\n",
      "Episode: 1620, Score: -107\n",
      "Episode: 1640, Score: -104\n",
      "Episode: 1660, Score: -13\n",
      "Episode: 1680, Score: -15\n",
      "Episode: 1700, Score: -13\n",
      "Episode: 1720, Score: -15\n",
      "Episode: 1740, Score: -14\n",
      "Episode: 1760, Score: -13\n",
      "Episode: 1780, Score: -13\n",
      "Episode: 1800, Score: -17\n",
      "Episode: 1820, Score: -13\n",
      "Episode: 1840, Score: -13\n",
      "Episode: 1860, Score: -13\n",
      "Episode: 1880, Score: -103\n",
      "Episode: 1900, Score: -15\n",
      "Episode: 1920, Score: -13\n",
      "Episode: 1940, Score: -116\n",
      "Episode: 1960, Score: -102\n",
      "Episode: 1980, Score: -15\n",
      "Episode: 2000, Score: -13\n",
      "Episode: 2020, Score: -13\n",
      "Episode: 2040, Score: -15\n",
      "Episode: 2060, Score: -13\n",
      "Episode: 2080, Score: -109\n",
      "Episode: 2100, Score: -15\n",
      "Episode: 2120, Score: -13\n",
      "Episode: 2140, Score: -13\n",
      "Episode: 2160, Score: -13\n",
      "Episode: 2180, Score: -13\n",
      "Episode: 2200, Score: -13\n",
      "Episode: 2220, Score: -13\n",
      "Episode: 2240, Score: -13\n",
      "Episode: 2260, Score: -100\n",
      "Episode: 2280, Score: -19\n",
      "Episode: 2300, Score: -20\n",
      "Episode: 2320, Score: -108\n",
      "Episode: 2340, Score: -13\n",
      "Episode: 2360, Score: -19\n",
      "Episode: 2380, Score: -15\n",
      "Episode: 2400, Score: -17\n",
      "Episode: 2420, Score: -13\n",
      "Episode: 2440, Score: -13\n",
      "Episode: 2460, Score: -13\n",
      "Episode: 2480, Score: -15\n",
      "Episode: 2500, Score: -13\n",
      "Episode: 2520, Score: -17\n",
      "Episode: 2540, Score: -100\n",
      "Episode: 2560, Score: -103\n",
      "Episode: 2580, Score: -14\n",
      "Episode: 2600, Score: -15\n",
      "Episode: 2620, Score: -17\n",
      "Episode: 2640, Score: -107\n",
      "Episode: 2660, Score: -13\n",
      "Episode: 2680, Score: -13\n",
      "Episode: 2700, Score: -15\n",
      "Episode: 2720, Score: -17\n",
      "Episode: 2740, Score: -109\n",
      "Episode: 2760, Score: -109\n",
      "Episode: 2780, Score: -13\n",
      "Episode: 2800, Score: -13\n",
      "Episode: 2820, Score: -13\n",
      "Episode: 2840, Score: -13\n",
      "Episode: 2860, Score: -13\n",
      "Episode: 2880, Score: -13\n",
      "Episode: 2900, Score: -13\n",
      "Episode: 2920, Score: -13\n",
      "Episode: 2940, Score: -15\n",
      "Episode: 2960, Score: -15\n",
      "Episode: 2980, Score: -15\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:50:44.395600Z",
     "start_time": "2024-10-11T07:50:44.310845Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "off_policy = False  # if True use off-policy q-learning update, if False, use on-policy SARSA update\n",
    "\n",
    "n_states = 40\n",
    "iter_max = 5000\n",
    "\n",
    "initial_lr = 1.0  # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "t_max = 10000\n",
    "eps = 0.1\n",
    "\n",
    "\n",
    "def run_episode(env, policy=None, render=False):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    for _ in range(t_max):\n",
    "        if render:\n",
    "            env.render()\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            a, b = obs_to_state(env, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    # we quantify the continuous state space into discrete space\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0]) / env_dx[0])\n",
    "    b = int((obs[1] - env_low[1]) / env_dx[1])\n",
    "    a = a if a < n_states else n_states - 1\n",
    "    b = b if b < n_states else n_states - 1\n",
    "    return a, b\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name = 'MountainCar-v0'\n",
    "    # env = gym.make(env_name, render_mode=\"human\")\n",
    "    env = gym.make(env_name)\n",
    "    # env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    if off_policy:\n",
    "        print('----- using Q Learning -----')\n",
    "    else:\n",
    "        print('------ using SARSA Learning ---')\n",
    "\n",
    "    q_table = np.zeros((n_states, n_states, 3))\n",
    "    for i in range(iter_max):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        ## eta: learning rate is decreased at each step\n",
    "        eta = max(min_lr, initial_lr * (0.85 ** (i // 100)))\n",
    "        for j in range(t_max):\n",
    "            a, b = obs_to_state(env, obs)\n",
    "            if np.random.uniform(0, 1) < eps:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                action = np.argmax(q_table[a][b])\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            # update q table\n",
    "            a_, b_ = obs_to_state(env, obs)\n",
    "            if off_policy:\n",
    "                # use q-learning update (off-policy learning)\n",
    "                q_table[a][b][action] = q_table[a][b][action] + eta * (\n",
    "                        reward + gamma * np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "            else:\n",
    "                # use SARSA update (on-policy learning)\n",
    "                # epsilon-greedy policy on Q again\n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action_ = np.random.choice(env.action_space.n)\n",
    "                else:\n",
    "                    action_ = np.argmax(q_table[a_][b_])\n",
    "                q_table[a][b][action] = q_table[a][b][action] + eta * (\n",
    "                        reward + gamma * q_table[a_][b_][action_] - q_table[a][b][action])\n",
    "            if done:\n",
    "                break\n",
    "        if i % 200 == 0:\n",
    "            print('Iteration #%d -- Total reward = %d.' % (i + 1, total_reward))\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "    print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "    # Animate it\n",
    "    for _ in range(2):\n",
    "        run_episode(env, solution_policy, True)\n",
    "    env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ using SARSA Learning ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/py/7yl145yn1z7bcykj9xn4xml00000gn/T/ipykernel_77914/1173397353.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     72\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m                 \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq_table\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m             \u001B[0mobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m             \u001B[0mtotal_reward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m             \u001B[0;31m# update q table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
