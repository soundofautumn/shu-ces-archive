{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验包括: \n",
    "* 基于朴素贝叶斯算法的言论过滤器\n",
    "* 基于朴素贝叶斯算法实现垃圾邮件过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.实验目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过本实验掌握朴素贝叶斯算法原理，熟悉朴素贝叶斯算法的简单应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实验知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 朴素贝叶斯算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* python 3.6.5\n",
    "* numpy 1.13.3\n",
    "* matplotlib 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点击屏幕右上方的下载实验数据模块，选择下载bayes_email.tgz到指定目录下，然后再依次选择点击上方的File->Open->Upload,上传刚才下载的数据集压缩包，再使用如下命令解压："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.742607Z",
     "start_time": "2024-12-20T08:04:00.573472Z"
    }
   },
   "source": [
    "!tar -zxvf bayes_email.tgz"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x bayes_email/\r\n",
      "x bayes_email/spam/\r\n",
      "x bayes_email/spam/1.txt\r\n",
      "x bayes_email/spam/10.txt\r\n",
      "x bayes_email/spam/11.txt\r\n",
      "x bayes_email/spam/12.txt\r\n",
      "x bayes_email/spam/13.txt\r\n",
      "x bayes_email/spam/14.txt\r\n",
      "x bayes_email/spam/15.txt\r\n",
      "x bayes_email/spam/16.txt\r\n",
      "x bayes_email/spam/17.txt\r\n",
      "x bayes_email/spam/18.txt\r\n",
      "x bayes_email/spam/19.txt\r\n",
      "x bayes_email/spam/2.txt\r\n",
      "x bayes_email/spam/20.txt\r\n",
      "x bayes_email/spam/21.txt\r\n",
      "x bayes_email/spam/22.txt\r\n",
      "x bayes_email/spam/23.txt\r\n",
      "x bayes_email/spam/24.txt\r\n",
      "x bayes_email/spam/25.txt\r\n",
      "x bayes_email/spam/3.txt\r\n",
      "x bayes_email/spam/4.txt\r\n",
      "x bayes_email/spam/5.txt\r\n",
      "x bayes_email/spam/6.txt\r\n",
      "x bayes_email/spam/7.txt\r\n",
      "x bayes_email/spam/8.txt\r\n",
      "x bayes_email/spam/9.txt\r\n",
      "x bayes_email/ham/\r\n",
      "x bayes_email/ham/1.txt\r\n",
      "x bayes_email/ham/10.txt\r\n",
      "x bayes_email/ham/11.txt\r\n",
      "x bayes_email/ham/12.txt\r\n",
      "x bayes_email/ham/13.txt\r\n",
      "x bayes_email/ham/14.txt\r\n",
      "x bayes_email/ham/15.txt\r\n",
      "x bayes_email/ham/16.txt\r\n",
      "x bayes_email/ham/17.txt\r\n",
      "x bayes_email/ham/18.txt\r\n",
      "x bayes_email/ham/19.txt\r\n",
      "x bayes_email/ham/2.txt\r\n",
      "x bayes_email/ham/20.txt\r\n",
      "x bayes_email/ham/21.txt\r\n",
      "x bayes_email/ham/22.txt\r\n",
      "x bayes_email/ham/23.txt\r\n",
      "x bayes_email/ham/24.txt\r\n",
      "x bayes_email/ham/25.txt\r\n",
      "x bayes_email/ham/3.txt\r\n",
      "x bayes_email/ham/4.txt\r\n",
      "x bayes_email/ham/5.txt\r\n",
      "x bayes_email/ham/6.txt\r\n",
      "x bayes_email/ham/7.txt\r\n",
      "x bayes_email/ham/8.txt\r\n",
      "x bayes_email/ham/9.txt\r\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.747889Z",
     "start_time": "2024-12-20T08:04:00.744441Z"
    }
   },
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【言论过滤器】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【言论过滤器】- 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以在线社区留言为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标志为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类型：侮辱类和非侮辱类，使用1和0分别表示。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【言论过滤器】- 词条切分和词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现所有文档中的单词，再决定将哪些单词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。简单起见，我们先假设已经将本文切分完毕，存放到列表中，并对词汇向量进行分类标注。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.754404Z",
     "start_time": "2024-12-20T08:04:00.749175Z"
    }
   },
   "source": [
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    函数说明:创建实验样本\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        postingList - 实验样本切分的词条\n",
    "        classVec - 类别标签向量\n",
    "    \"\"\"\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVec"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【言论过滤器】- 生成词条向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续编写代码，并将切分好的词条转换为词条向量。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.778753Z",
     "start_time": "2024-12-20T08:04:00.763584Z"
    }
   },
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    函数说明: 根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0\n",
    "    Parameters:\n",
    "        vocabList - createVocabList返回的列表\n",
    "        inputSet - 切分的词条列表\n",
    "    Returns:\n",
    "        returnVec - 文档向量,词集模型\n",
    "    \"\"\"\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    函数说明: 将切分的实验样本词条整理成不重复的词条列表，也就是词汇表\n",
    "\n",
    "    Parameters:\n",
    "        dataSet - 整理的样本数据集\n",
    "    Returns:\n",
    "        vocabSet - 返回不重复的词条列表，也就是词汇表\n",
    "    \"\"\"\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【言论过滤器】-训练朴素贝叶斯分类器 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过词条向量训练朴素贝叶斯分类器。\n",
    "\n",
    "Tips：\n",
    "* 利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率。如果其中有一个概率值为0，那么最后的乘积也为0。为降低这种影响，可以将所有词的出现数初始化为1， 并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)，是比较常用的平滑方法。\n",
    "\n",
    "* 下溢问题。由于太多很小的数相乘会导致下溢问题。两个小数相乘，越乘越小，这样就造成了下溢出。在程序中，许多很小的数相乘，最后四舍五入计算结果可能就变成0。为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.807280Z",
     "start_time": "2024-12-20T08:04:00.797318Z"
    }
   },
   "source": [
    "def trainNB(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    函数说明:朴素贝叶斯分类器训练函数\n",
    "\n",
    "    Parameters:\n",
    "        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵\n",
    "        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec\n",
    "    Returns:\n",
    "        p0Vect - 非的条件概率数组\n",
    "        p1Vect - 侮辱类的条件概率数组\n",
    "        pAbusive - 文档属于侮辱类的概率\n",
    "    \"\"\"\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vec = p1Num / p1Denom\n",
    "    p0Vec = p0Num / p0Denom\n",
    "    return p0Vec, p1Vec, pAbusive"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【言论过滤器】- 使用分类器进行分类"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.821060Z",
     "start_time": "2024-12-20T08:04:00.811071Z"
    }
   },
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    函数说明:朴素贝叶斯分类器分类函数\n",
    "\n",
    "    Parameters:\n",
    "        vec2Classify - 待分类的词条数组\n",
    "        p0Vec - 非侮辱类的条件概率数组\n",
    "        p1Vec -侮辱类的条件概率数组\n",
    "        pClass1 - 文档属于侮辱类的概率\n",
    "    Returns:\n",
    "        0 - 属于非侮辱类\n",
    "        1 - 属于侮辱类\n",
    "    \"\"\"\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def testingNB():\n",
    "    \"\"\"\n",
    "    函数说明:测试朴素贝叶斯分类器\n",
    "    测试样本1：['love', 'my', 'dalmation']\n",
    "    测试样本2：['stupid', 'garbage']\n",
    "    Parameters:\n",
    "        无\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ],
   "outputs": [],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.831863Z",
     "start_time": "2024-12-20T08:04:00.823786Z"
    }
   },
   "source": [
    "if __name__ == '__main__':\n",
    "    testingNB()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【垃圾邮件过滤】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【垃圾邮件过滤】- 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用朴素贝叶斯解决一些现实生活中的问题时，需要先从文本内容得到字符串列表，然后生成词向量。本实验中，我们将了解朴素贝叶斯的一个最著名的应用：电子邮件垃圾过滤。首先看一下使用朴素贝叶斯对电子邮件进行分类的步骤：\n",
    "\n",
    "* 收集数据：提供文本文件。\n",
    "* 准备数据：将文本文件解析成词条向量。\n",
    "* 分析数据：检查词条确保解析的正确性。\n",
    "* 训练算法：使用我们之前建立的trainNB()函数。\n",
    "* 测试算法：使用classifyNB()函数，并构建一个新的测试函数来计算文档集的错误率。\n",
    "* 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。\n",
    "\n",
    "数据集路径为：bayes_email，该目录下有两个文件夹：ham和spam，其中spam文件下的txt文件为垃圾邮件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据示例\n",
    "![](dataIntro_bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【垃圾邮件过滤】- 文本切分 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于英文文本，我们可以以非字母、非数字作为符号进行切分，使用split函数即可。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.835602Z",
     "start_time": "2024-12-20T08:04:00.833071Z"
    }
   },
   "source": [
    "def textParse(bigString):\n",
    "    \"\"\"\n",
    "    函数说明:接收一个字符串并将其解析为字符串列表\n",
    "\n",
    "    Parameters:\n",
    "        bigString - 字符串\n",
    "    Returns:\n",
    "        字符串列表(除了单个字母，例如大写的I，其它单词变成小写，同时过滤长度小于3的字符串)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\w*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤：【垃圾邮件过滤】- 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据词汇表，将每个文本向量化。将数据集分为训练集和测试集，使用交叉验证的方式测试朴素贝叶斯分类器的准确性。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.854882Z",
     "start_time": "2024-12-20T08:04:00.843675Z"
    }
   },
   "source": [
    "def spamTest():\n",
    "    \"\"\"\n",
    "    函数说明: 将数据集分为训练集和测试集，使用交叉验证的方式测试朴素贝叶斯分类器的准确性\n",
    "    \"\"\"\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1, 26):\n",
    "        # 导入并解析文本文件\n",
    "        wordList = textParse(open('bayes_email/spam/%d.txt' % i, encoding='cp1252').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('bayes_email/ham/%d.txt' % i, encoding='cp1252').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    # 随机取10个邮件用来测试\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del (trainingSet[randIndex])\n",
    "\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "\n",
    "    p0V, p1V, pSpam = trainNB(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is: ', float(errorCount) / len(testSet))"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.882116Z",
     "start_time": "2024-12-20T08:04:00.873648Z"
    }
   },
   "source": [
    "if __name__ == '__main__':\n",
    "    spamTest()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.5\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:04:00.892117Z",
     "start_time": "2024-12-20T08:04:00.888765Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 107
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
