{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "489082009c2d4a93b494bc8451bd0377",
            "5e2f573cbe1b433ab402b2017c082661",
            "9adfced271554de5aeaae54d220e61fc",
            "08a4eb4d2ba44906a3f6798eb4d06427",
            "7f699a0d7e0d4c05b057975273753817",
            "b24a0c17b0ae4e53a7d5701453a2a79f",
            "043b305d17354333a0275f60256e248e",
            "89518f8b77ca4522bd56412dafa647a6",
            "e72778867f674f199074433bb05ef833",
            "223046e86c9b493789c43d5ea53c12f3",
            "bfe656d2615a49a8b52187b1dab7d716",
            "12e1efa0735a4eb486bf39671b3e6d17",
            "75731b7033a84348af28206a7edaf5c7",
            "66bda0755f7b49f8ab97a1c0683cd8f7",
            "f0e2f48b59df4745a40c8cb92c43517e",
            "968e3e28fd8844ac8d3523358dd9fcec",
            "0c7b124859c44d84a2269d818e4aadaf"
          ]
        },
        "id": "Ipr5r1AbNhDi",
        "outputId": "2bdbf935-013e-4c34-8f4c-9019797f653d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "489082009c2d4a93b494bc8451bd0377",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4VbDUr1NhDk",
        "outputId": "295497d4-f804-450b-c0c5-846a76e2edfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/competitions/data/download-all/cassava-leaf-disease-classification...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.76G/5.76G [00:30<00:00, 204MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/abhinand05/vit-base-models-pretrained-pytorch?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.20G/1.20G [00:06<00:00, 191MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/abhinand05/vittutorialillustrations?dataset_version_number=5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.01M/1.01M [00:00<00:00, 124MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "cassava_leaf_disease_classification_path = kagglehub.competition_download('cassava-leaf-disease-classification')\n",
        "abhinand05_vit_base_models_pretrained_pytorch_path = kagglehub.dataset_download('abhinand05/vit-base-models-pretrained-pytorch')\n",
        "abhinand05_vittutorialillustrations_path = kagglehub.dataset_download('abhinand05/vittutorialillustrations')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "leJvx1lBNhDk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !cp -r ../input/vittutorialillustrations/* ./\n",
        "\n",
        "# !pip install nb_black\n",
        "# %load_ext nb_black"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALGTpD--NhDk"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook essentially has two parts as seen from the title.\n",
        "\n",
        "<a href=\"#Vision-Transformers:-A-gentle-introduction\">1. Vision Transformer: A gentle introduction</a> <br>\n",
        "<a href=\"#Vision-Transformer-Implementation-in-PyTorch\">2. Implementation in PyTorch</a>\n",
        "\n",
        "**I'll briefly try to explain the fundamental ideas behind Vision Transformers and how it works before getting into the implementation of ViT in PyTorch for this competition.**\n",
        "\n",
        "So if you are only interested in code, you can feel free to skip straight ahead to the second section of this notebook. The implementation isn't a whole lot different, thanks to [rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models) library which contains all the model implementations including the pretrained weights for us to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtwWEakcNhDl"
      },
      "source": [
        "# <font size=4 color='blue'>If you find this notebook useful, leave an upvote, that motivates me to write more such notebooks.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFJMhD5ONhDl"
      },
      "source": [
        "# Vision Transformers: A gentle introduction\n",
        "\n",
        "Vision Transformers were first introduced in the paper [AN IMAGE IS WORTH 16X16 WORDS:\n",
        "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929.pdf) by the Google Brain team in late October 2020.\n",
        "\n",
        "To understand how ViT works, obviously you must have prior knowledge about how Transformers work and what problems it solved. I'll briefly introduce you to how transformers work before getting into the details of the topic at hand - ViT.\n",
        "\n",
        "![ViT-Illustration](vision-transformer.png)\n",
        "\n",
        "If you are new to NLP and interested in learning more about the transformer models and get a fair intuition of how they actually work, I recomment checking out the fantastic blog posts of [Jay Allamar](https://jalammar.github.io/). (The image above was also inspired from one of his blog posts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mEd6ZtfNhDl"
      },
      "source": [
        "## Transformers: A brief overview\n",
        "\n",
        "> **If you already understand Transformers, feel free to skip ahead to the next section.**\n",
        "\n",
        "Transformer models well and truly revolutionized Natural Language Processing as we know. When they were first introduced they broke multiple NLP records and were pushing the then State of the Art. Now, they have become a de-facto standard for modern NLP tasks and they bring spectacular performance gains when compared to the previous generation of models like LSTMs and GRUs.  \n",
        "\n",
        "By far the most important paper that transformed the NLP landscape is the [\"Attention is all you need\"](https://arxiv.org/pdf/1706.03762.pdf) paper. The transformer architecture was introduced in this paper.\n",
        "\n",
        "### **Motivations:**\n",
        "\n",
        "The existing models at that time for sequence and NLP tasks mostly involved RNNs. **The problem with these networks were that they couldn't capture long term dependencies.**\n",
        "\n",
        "LSTMs and GRUs - variants of RNNs were capable of capturing the dependencies but it is also limited.\n",
        "\n",
        "So, the main inspiration behind the transformer was to get rid of this recurrence and still end up capturing almost all the dependencies, to be precise global dependencies, yes the reference window of tranformers is full-range. This was achieved using a variant of attention mechanism called *self-attention* (multi-headed) which is very important for their success. One other advantage of Tranformer models are that they are highly parallelizable.  \n",
        "\n",
        "\n",
        "### Transformer Architecture\n",
        "**Note: The architecture diagrams are annotated with the corresponding step in the explanation.**\n",
        "\n",
        "![TranformerArchitecture](transformer-arch.png)\n",
        "\n",
        "- Transformer has two parts, the decoder which is on the left side on the above diagram and the encoder which is on the right.\n",
        "- Imagine we are doing machine translation for now.\n",
        "- The encoder takes the input data (sentence), and produces an intermediate representation of the input.\n",
        "- The decoder decodes this intermediate representation step by step and generates the output. The difference however is in how it is doing this.\n",
        "- Understanding the Encoder section is enough for ViT.\n",
        "\n",
        "> **Note: The explanations here are more about the intuition behind the architectures. For more mathematical details check out the respective research papers instead.**\n",
        "\n",
        "### Tranformers: Step by step overview\n",
        "**(1)** The input data first gets embedded into a vector. The embedding layer helps us grab a learned vector representation for each word.\n",
        "\n",
        "**(2)** In the next stage a positional encoding is injected into the input embeddings. This is because a transformer has no idea about the order of the sequence that is being passed as input - for example a sentence.\n",
        "\n",
        "**(3)** Now the multi-headed attention is where things get a little different.\n",
        "\n",
        "**Multi-headed-attention architecture:**\n",
        "![multi-headed-attn](multi-headed-attention.png)\n",
        "\n",
        "**(4)** Multi-Headed Attention consists of three learnable vectors. Query, Key and Value vectors. The motivation of this reportedly comes from information retrival where you search (query) and the search engine compares your query with a key and responds with a value.\n",
        "\n",
        "**(5)** The Q and K representations undergo a dot product matrix multiplication to produce a score matrix which represents how much a word has to attend to every other word. Higher score means more attention and vice-versa.\n",
        "\n",
        "**(6)** Then the Score matrix is scaled down according to the dimensions of the Q and K vectors. This is to ensure more stable gradients as multiplication can have exploding effects.\n",
        "\n",
        "(We'll discuss the mask part when we reach the decoder section)\n",
        "\n",
        "**(7)** Next the Score matrix is softmaxed to turn attention scores into probabilities. Obviously higher scores are heightened and lower scores are depressed. This ensures the model to be confident on which words to attend to.\n",
        "\n",
        "**(8)** Then the resultant matrix with probabilites is multiplied with the value vector. This will make the higher probaility scores the model has learned to be more important. The low scoring words will effectively drown out to become irrelevant.\n",
        "\n",
        "**(9)** Then, the concatenated output of QK and V vectors are fed into the Linear layer to process further.\n",
        "\n",
        "**(10)** Self-Attention is performed for each word in the sequence. Since one doesn't depend on the other a copy of the self attention module can be used to process everything simultaneously making this **multi-headed**.\n",
        "\n",
        "**(11)** Then the output value vectors are concatenated and added to the residual connection coming from the input layer and then the resultant respresentation is passed into a *LayerNorm* for normalization. (Residual connection help gradients flow through the network and LayernNorm helps reduce the training time by a small fraction and stabilize the network)\n",
        "\n",
        "**(12)** Further, the output is passed into a point-wise feed forward network to obtain an even richer representation.  \n",
        "\n",
        "**(13)** The outputs are again Layer-normed and residuals are added from the previous layer.\n",
        "\n",
        "\n",
        "**Note: This wraps up the encoder section and trust me this is enough to fully understand Vision Transformer. I'll be largely leaving it up to you to understand the decoder part as it is very similar to the encoding layer.**\n",
        "\n",
        "**(14)** The output from the encoder along with the inputs (if any) from the previous time steps/words are fed into the decoder where the outputs undergo masked-multi headed attention before being fed into the next attention layer along with the output from encoder.\n",
        "\n",
        "**(15)** Masked multi headed attention is necessary because the network shouldn't have any visibility into the words that are to come later in the sequence while decoding, to ensure there is no leak. This is done by masking the entries of words that come later in the series in the Score matrix. Current and previous words in the sequence are added with 1 and the future word scores are added with `-inf`. This ensures the future words in the series get drowned out into 0 when performing softmax to obtain the probabilities, while the rest are retained.\n",
        "\n",
        "**(16)** There are residual connections here as well, to improve the flow of gradients. Finally the output is sent to a Linear layer and softmaxed to obtain the outputs in probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjo5sJFeNhDl"
      },
      "source": [
        "## How Vision Tranformers works?\n",
        "\n",
        "Now that we have covered transformers' internal working at a high level, we are finally ready to tackle Vision Tranformers.\n",
        "\n",
        "Applying Transformers on images was always going to be a challenge for the following reasons,\n",
        "- Unlike words/sentences/paragraphs, images contain much much more information in them basically in form of pixels.\n",
        "- It would be very hard, even with current hardware to attend to every other pixel in the image.\n",
        "- Instead, a popular alternative was to use localized attention.\n",
        "- In fact CNNs do something very similar through convolutions and the receptive field essentially grows bigger as we go deeper into the model's layers, but Tranformers were always going to be computationally more expensive than CNNs because of the' nature of Transformers. And of course, we know how incredibly much CNNs have contributed to the current advancements in Computer Vision.\n",
        "\n",
        "Google researchers have proposed something different in their paper than can possibly be the next big step in Computer Vision. They show that the reliance on CNNs may not be necessary anymore. So, let's dive right in and explore more about Vision Transformers.\n",
        "\n",
        "\n",
        "### Vision Transformer Architecture\n",
        "\n",
        "![vit-architecture](vit-arch.png)\n",
        "\n",
        "**(1)** They are only using the Encoder part of the transformer but the difference is in how they are feeding the images into the network.\n",
        "\n",
        "**(2)** They are breaking down the image into fixed size patches. So one of these patches can be of dimension 16x16 or 32x32 as proposed in the paper. More patches means more simpler it is to train these networks as the patches themselves get smaller. Hence we have that in the title - \"An Image is worth 16x16 words\".\n",
        "\n",
        "**(3)** The patches are then unrolled (flattened) and sent for further processing into the network.\n",
        "\n",
        "**(4)** Unlike NNs here the model has no idea whatsoever about the position of the samples in the sequence, here each sample is a patch from the input image. So the image is fed **along with a positional embedding vector** and into the encoder. One thing to note here is the positional embeddings are also learnable so you don't actually feed hard-coded vectors w.r.t to their positions.\n",
        "\n",
        "**(5)** There is also a special token at the start just like BERT.\n",
        "\n",
        "**(6)** So each image patch is first unrolled (flattened) into a big vector and gets multiplied with an embedding matrix which is also learnable, creating embedded patches. And these embedded patches are combined with the positional embedding vector and that gets fed into the Tranformer.\n",
        "\n",
        "> **Note: From here everything is just the same as a standard transformer**\n",
        "\n",
        "**(7)** With the only difference being, instead of a decoder the output from the encoder is passed directly into a Feed Forward Neural Network to obtain the classification output.\n",
        "\n",
        "### Things to note:\n",
        "- The paper ALMOST completely neglects Convolutions.\n",
        "- They are however using a couple of variants of ViT in which Convolutional embeddings of image patches are used. But that doesn't seem to impact performance much.  \n",
        "- Vision Transformers, at the time of writing this are topping Image Classification benchmarks on ImageNet.\n",
        "\n",
        "<img src=\"benchmarks-chart.png\" width=\"700\">\n",
        "<!-- ![BenchmarksChart](benchmarks-charpng) -->\n",
        "\n",
        "- There are a lot more interesting things in this paper but the one thing that stands out for me and potentially shows the power of transformers over CNNs is illustrated in the image below which shows the attention distance with respect to the layers.\n",
        "\n",
        "\n",
        "<img src=\"attn-distance.png\" width=\"300\" height=\"300\">\n",
        "<br>\n",
        "\n",
        "- The graph above suggests that the Transformers are already capable of paying attention to  regions that are far apart right from the starting layers of the network which is a pretty significant gain the Transformers bring over CNNs which has a finite receptive field at the start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlctNettNhDm"
      },
      "source": [
        "# <font size=4 color='blue'>If you find this notebook useful, leave an upvote, that motivates me to write more such notebooks.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A5R-DbXNhDm"
      },
      "source": [
        "## Vision Transformer Implementation in PyTorch\n",
        "Now that you understand vision transformers, let's build a baseline model for [this competition](https://www.kaggle.com/c/cassava-leaf-disease-classification)\n",
        "\n",
        "First lets install torch-xla to be able to use the TPU and torch-image-models (timm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "nW-3rGfLNhDm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version 1.7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZISyl_sPYND",
        "outputId": "33fdb191-e02d-400f-bcd5-d80eb7d6ae14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cpu)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.15\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0-JLiRjFNhDm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "import timm\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import model_selection, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ9Ud9h3NhDm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# For parallelization in TPUs\n",
        "# 设置TPU相关环境变量\n",
        "os.environ[\"XLA_USE_BF16\"] = \"1\"  # 使用BF16精度\n",
        "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"  # 设置张量分配器最大大小"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLxrNsntNhDm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 设置随机种子函数,确保结果可复现\n",
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproductibility of results\n",
        "\n",
        "    Arguments:\n",
        "        seed {int} -- Number of the seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "seed_everything(1001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QbNZj5FEPqIm",
        "outputId": "a49387ce-bb8c-47e2-e561-0657e9df0a95"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.cache/kagglehub/competitions/cassava-leaf-disease-classification'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cassava_leaf_disease_classification_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6vBxowDdPxth",
        "outputId": "3635a7ad-91f7-4eaf-fe61-79f31a583f6b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/abhinand05/vit-base-models-pretrained-pytorch/versions/1'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abhinand05_vit_base_models_pretrained_pytorch_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zmPkfvVP26z",
        "outputId": "8272876e-060b-49eb-b88a-4bf394fc211b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jx_vit_base_p16_224-80ecf9dd.pth  jx_vit_base_p32_384-830016f5.pth\n",
            "jx_vit_base_p16_384-83fb41ba.pth  jx_vit_base_patch16_384_in21k-0243c7d9.pth\n"
          ]
        }
      ],
      "source": [
        "!ls /root/.cache/kagglehub/datasets/abhinand05/vit-base-models-pretrained-pytorch/versions/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2b_r9MwBNhDm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# general global variables\n",
        "# DATA_PATH = \"../input/cassava-leaf-disease-classification\"\n",
        "# TRAIN_PATH = \"../input/cassava-leaf-disease-classification/train_images/\"\n",
        "# TEST_PATH = \"../input/cassava-leaf-disease-classification/test_images/\"\n",
        "# MODEL_PATH = (\n",
        "#     \"../input/vit-base-models-pretrained-pytorch/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "# )\n",
        "DATA_PATH = cassava_leaf_disease_classification_path\n",
        "TRAIN_PATH = cassava_leaf_disease_classification_path + \"/train_images/\"\n",
        "TEST_PATH = cassava_leaf_disease_classification_path + \"/test_images/\"\n",
        "MODEL_PATH = (\n",
        "    abhinand05_vit_base_models_pretrained_pytorch_path\n",
        "    + \"/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        ")\n",
        "\n",
        "# model specific global variables\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "LR = 2e-05\n",
        "GAMMA = 0.7\n",
        "N_EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3U17n24GNhDn",
        "outputId": "3b9f4944-89fe-4e84-bd83-1dc4307568fd",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 21397,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21397,\n        \"samples\": [\n          \"2615227158.jpg\",\n          \"1277648239.jpg\",\n          \"2305895487.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-965ae061-31db-4468-b654-9bb9f4e58fdf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000015157.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000201771.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100042118.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000723321.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000812911.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-965ae061-31db-4468-b654-9bb9f4e58fdf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-965ae061-31db-4468-b654-9bb9f4e58fdf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-965ae061-31db-4468-b654-9bb9f4e58fdf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a2468133-b675-4878-88c7-8b24e7bb00af\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2468133-b675-4878-88c7-8b24e7bb00af')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a2468133-b675-4878-88c7-8b24e7bb00af button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         image_id  label\n",
              "0  1000015157.jpg      0\n",
              "1  1000201771.jpg      3\n",
              "2   100042118.jpg      1\n",
              "3  1000723321.jpg      1\n",
              "4  1000812911.jpg      3"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShYnMir6NhDn",
        "outputId": "26651475-bd75-4e5c-d78e-e3c0c9ca5229",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21397 entries, 0 to 21396\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   image_id  21397 non-null  object\n",
            " 1   label     21397 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 334.5+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "jZbtWZ0tNhDn",
        "outputId": "800a2b8e-c385-4c08-ca49-3fc2b5cebeca",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='label'>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.label.value_counts().plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "iKvIYYbTNhDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = df.sample(frac=0.1).reset_index(drop=True)\n",
        "train_df, valid_df = model_selection.train_test_split(\n",
        "    df, test_size=0.1, random_state=42, stratify=df.label.values\n",
        ")\n",
        "\n",
        "#减少训练集大小\n",
        "# train_df = train_df.sample(frac=0.5, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1aeGLST0Vv8W",
        "outputId": "7cc05438-9c11-4762-e673-67c7dfa8ae89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 192,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 192,\n        \"samples\": [\n          \"1570477723.jpg\",\n          \"2186113594.jpg\",\n          \"3115800767.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "train_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-212bab80-1f6a-4198-8218-e82c1b517a1c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>1272495783.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1733354827.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>526290781.jpg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>33304341.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>815702740.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>2905887778.jpg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2750295170.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3877043596.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>586687554.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>2324786048.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>192 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-212bab80-1f6a-4198-8218-e82c1b517a1c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-212bab80-1f6a-4198-8218-e82c1b517a1c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-212bab80-1f6a-4198-8218-e82c1b517a1c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5fdef890-a1c6-46f9-80a2-61b993027ce7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fdef890-a1c6-46f9-80a2-61b993027ce7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5fdef890-a1c6-46f9-80a2-61b993027ce7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8e43e70d-757b-48fb-925d-df168e505e6b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8e43e70d-757b-48fb-925d-df168e505e6b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           image_id  label\n",
              "173  1272495783.jpg      0\n",
              "76   1733354827.jpg      3\n",
              "50    526290781.jpg      4\n",
              "195    33304341.jpg      1\n",
              "29    815702740.jpg      3\n",
              "..              ...    ...\n",
              "54   2905887778.jpg      4\n",
              "39   2750295170.jpg      3\n",
              "18   3877043596.jpg      3\n",
              "64    586687554.jpg      3\n",
              "98   2324786048.jpg      3\n",
              "\n",
              "[192 rows x 2 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urnj7S5KNhDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CassavaDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Helper Class to create the pytorch dataset\n",
        "    自定义数据集类\n",
        "    参数:\n",
        "        df: 数据标签DataFrame\n",
        "        data_path: 数据根目录\n",
        "        mode: train/test模式\n",
        "        transforms: 数据增强transforms\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n",
        "        super().__init__()\n",
        "        self.df_data = df.values\n",
        "        self.data_path = data_path\n",
        "        self.transforms = transforms\n",
        "        self.mode = mode\n",
        "        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name, label = self.df_data[index]\n",
        "        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(img)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6c1lOwmNNhDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# create image augmentations\n",
        "transforms_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(p=0.3),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        transforms.RandomResizedCrop(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transforms_valid = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1GGcQwSNhDn",
        "outputId": "2a59ff13-5f16-487d-9e9f-41c46c2792f4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Vision Transformer Models: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['vit_base_mci_224',\n",
              " 'vit_base_patch8_224',\n",
              " 'vit_base_patch14_dinov2',\n",
              " 'vit_base_patch14_reg4_dinov2',\n",
              " 'vit_base_patch16_18x2_224',\n",
              " 'vit_base_patch16_224',\n",
              " 'vit_base_patch16_224_miil',\n",
              " 'vit_base_patch16_384',\n",
              " 'vit_base_patch16_clip_224',\n",
              " 'vit_base_patch16_clip_384',\n",
              " 'vit_base_patch16_clip_quickgelu_224',\n",
              " 'vit_base_patch16_gap_224',\n",
              " 'vit_base_patch16_plus_240',\n",
              " 'vit_base_patch16_plus_clip_240',\n",
              " 'vit_base_patch16_reg4_gap_256',\n",
              " 'vit_base_patch16_rope_reg1_gap_256',\n",
              " 'vit_base_patch16_rpn_224',\n",
              " 'vit_base_patch16_siglip_224',\n",
              " 'vit_base_patch16_siglip_256',\n",
              " 'vit_base_patch16_siglip_384',\n",
              " 'vit_base_patch16_siglip_512',\n",
              " 'vit_base_patch16_siglip_gap_224',\n",
              " 'vit_base_patch16_siglip_gap_256',\n",
              " 'vit_base_patch16_siglip_gap_384',\n",
              " 'vit_base_patch16_siglip_gap_512',\n",
              " 'vit_base_patch16_xp_224',\n",
              " 'vit_base_patch32_224',\n",
              " 'vit_base_patch32_384',\n",
              " 'vit_base_patch32_clip_224',\n",
              " 'vit_base_patch32_clip_256',\n",
              " 'vit_base_patch32_clip_384',\n",
              " 'vit_base_patch32_clip_448',\n",
              " 'vit_base_patch32_clip_quickgelu_224',\n",
              " 'vit_base_patch32_plus_256',\n",
              " 'vit_base_patch32_siglip_256',\n",
              " 'vit_base_patch32_siglip_gap_256',\n",
              " 'vit_base_r26_s32_224',\n",
              " 'vit_base_r50_s16_224',\n",
              " 'vit_base_r50_s16_384',\n",
              " 'vit_base_resnet26d_224',\n",
              " 'vit_base_resnet50d_224',\n",
              " 'vit_betwixt_patch16_gap_256',\n",
              " 'vit_betwixt_patch16_reg1_gap_256',\n",
              " 'vit_betwixt_patch16_reg4_gap_256',\n",
              " 'vit_betwixt_patch16_reg4_gap_384',\n",
              " 'vit_betwixt_patch16_rope_reg4_gap_256',\n",
              " 'vit_betwixt_patch32_clip_224',\n",
              " 'vit_giant_patch14_224',\n",
              " 'vit_giant_patch14_clip_224',\n",
              " 'vit_giant_patch14_dinov2',\n",
              " 'vit_giant_patch14_reg4_dinov2',\n",
              " 'vit_giant_patch16_gap_224',\n",
              " 'vit_giantopt_patch16_siglip_256',\n",
              " 'vit_giantopt_patch16_siglip_384',\n",
              " 'vit_giantopt_patch16_siglip_gap_256',\n",
              " 'vit_giantopt_patch16_siglip_gap_384',\n",
              " 'vit_gigantic_patch14_224',\n",
              " 'vit_gigantic_patch14_clip_224',\n",
              " 'vit_gigantic_patch14_clip_quickgelu_224',\n",
              " 'vit_huge_patch14_224',\n",
              " 'vit_huge_patch14_clip_224',\n",
              " 'vit_huge_patch14_clip_336',\n",
              " 'vit_huge_patch14_clip_378',\n",
              " 'vit_huge_patch14_clip_quickgelu_224',\n",
              " 'vit_huge_patch14_clip_quickgelu_378',\n",
              " 'vit_huge_patch14_gap_224',\n",
              " 'vit_huge_patch14_xp_224',\n",
              " 'vit_huge_patch16_gap_448',\n",
              " 'vit_intern300m_patch14_448',\n",
              " 'vit_large_patch14_224',\n",
              " 'vit_large_patch14_clip_224',\n",
              " 'vit_large_patch14_clip_336',\n",
              " 'vit_large_patch14_clip_quickgelu_224',\n",
              " 'vit_large_patch14_clip_quickgelu_336',\n",
              " 'vit_large_patch14_dinov2',\n",
              " 'vit_large_patch14_reg4_dinov2',\n",
              " 'vit_large_patch14_xp_224',\n",
              " 'vit_large_patch16_224',\n",
              " 'vit_large_patch16_384',\n",
              " 'vit_large_patch16_siglip_256',\n",
              " 'vit_large_patch16_siglip_384',\n",
              " 'vit_large_patch16_siglip_512',\n",
              " 'vit_large_patch16_siglip_gap_256',\n",
              " 'vit_large_patch16_siglip_gap_384',\n",
              " 'vit_large_patch16_siglip_gap_512',\n",
              " 'vit_large_patch32_224',\n",
              " 'vit_large_patch32_384',\n",
              " 'vit_large_r50_s32_224',\n",
              " 'vit_large_r50_s32_384',\n",
              " 'vit_little_patch16_reg1_gap_256',\n",
              " 'vit_little_patch16_reg4_gap_256',\n",
              " 'vit_medium_patch16_clip_224',\n",
              " 'vit_medium_patch16_gap_240',\n",
              " 'vit_medium_patch16_gap_256',\n",
              " 'vit_medium_patch16_gap_384',\n",
              " 'vit_medium_patch16_reg1_gap_256',\n",
              " 'vit_medium_patch16_reg4_gap_256',\n",
              " 'vit_medium_patch16_rope_reg1_gap_256',\n",
              " 'vit_medium_patch32_clip_224',\n",
              " 'vit_mediumd_patch16_reg4_gap_256',\n",
              " 'vit_mediumd_patch16_reg4_gap_384',\n",
              " 'vit_mediumd_patch16_rope_reg1_gap_256',\n",
              " 'vit_pwee_patch16_reg1_gap_256',\n",
              " 'vit_relpos_base_patch16_224',\n",
              " 'vit_relpos_base_patch16_cls_224',\n",
              " 'vit_relpos_base_patch16_clsgap_224',\n",
              " 'vit_relpos_base_patch16_plus_240',\n",
              " 'vit_relpos_base_patch16_rpn_224',\n",
              " 'vit_relpos_base_patch32_plus_rpn_256',\n",
              " 'vit_relpos_medium_patch16_224',\n",
              " 'vit_relpos_medium_patch16_cls_224',\n",
              " 'vit_relpos_medium_patch16_rpn_224',\n",
              " 'vit_relpos_small_patch16_224',\n",
              " 'vit_relpos_small_patch16_rpn_224',\n",
              " 'vit_small_patch8_224',\n",
              " 'vit_small_patch14_dinov2',\n",
              " 'vit_small_patch14_reg4_dinov2',\n",
              " 'vit_small_patch16_18x2_224',\n",
              " 'vit_small_patch16_36x1_224',\n",
              " 'vit_small_patch16_224',\n",
              " 'vit_small_patch16_384',\n",
              " 'vit_small_patch32_224',\n",
              " 'vit_small_patch32_384',\n",
              " 'vit_small_r26_s32_224',\n",
              " 'vit_small_r26_s32_384',\n",
              " 'vit_small_resnet26d_224',\n",
              " 'vit_small_resnet50d_s16_224',\n",
              " 'vit_so150m2_patch16_reg1_gap_256',\n",
              " 'vit_so150m2_patch16_reg1_gap_384',\n",
              " 'vit_so150m2_patch16_reg1_gap_448',\n",
              " 'vit_so150m_patch16_reg4_gap_256',\n",
              " 'vit_so150m_patch16_reg4_gap_384',\n",
              " 'vit_so150m_patch16_reg4_map_256',\n",
              " 'vit_so400m_patch14_siglip_224',\n",
              " 'vit_so400m_patch14_siglip_378',\n",
              " 'vit_so400m_patch14_siglip_384',\n",
              " 'vit_so400m_patch14_siglip_gap_224',\n",
              " 'vit_so400m_patch14_siglip_gap_378',\n",
              " 'vit_so400m_patch14_siglip_gap_384',\n",
              " 'vit_so400m_patch14_siglip_gap_448',\n",
              " 'vit_so400m_patch14_siglip_gap_896',\n",
              " 'vit_so400m_patch16_siglip_256',\n",
              " 'vit_so400m_patch16_siglip_384',\n",
              " 'vit_so400m_patch16_siglip_512',\n",
              " 'vit_so400m_patch16_siglip_gap_256',\n",
              " 'vit_so400m_patch16_siglip_gap_384',\n",
              " 'vit_so400m_patch16_siglip_gap_512',\n",
              " 'vit_srelpos_medium_patch16_224',\n",
              " 'vit_srelpos_small_patch16_224',\n",
              " 'vit_tiny_patch16_224',\n",
              " 'vit_tiny_patch16_384',\n",
              " 'vit_tiny_r_s16_p8_224',\n",
              " 'vit_tiny_r_s16_p8_384',\n",
              " 'vit_wee_patch16_reg1_gap_256',\n",
              " 'vit_xsmall_patch16_clip_224',\n",
              " 'vitamin_base_224',\n",
              " 'vitamin_large2_224',\n",
              " 'vitamin_large2_256',\n",
              " 'vitamin_large2_336',\n",
              " 'vitamin_large2_384',\n",
              " 'vitamin_large_224',\n",
              " 'vitamin_large_256',\n",
              " 'vitamin_large_336',\n",
              " 'vitamin_large_384',\n",
              " 'vitamin_small_224',\n",
              " 'vitamin_xlarge_256',\n",
              " 'vitamin_xlarge_336',\n",
              " 'vitamin_xlarge_384']"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Available Vision Transformer Models: \")\n",
        "timm.list_models(\"vit*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe1rhft2NhDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ViTBase16(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer实现类\n",
        "    参数:\n",
        "        n_classes: 分类数\n",
        "        pretrained: 是否加载预训练权重\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes, pretrained=False):\n",
        "\n",
        "        super(ViTBase16, self).__init__()\n",
        "        # 创建基础ViT模型\n",
        "        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n",
        "        if pretrained:\n",
        "            self.model.load_state_dict(torch.load(MODEL_PATH))\n",
        "        # 修改分类头\n",
        "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
        "        # keep track of training loss\n",
        "        epoch_loss = 0.0\n",
        "        epoch_accuracy = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        self.model.train()\n",
        "        for i, (data, target) in enumerate(train_loader):\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if device.type == \"cuda\":\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            elif device.type == \"xla\":\n",
        "                data = data.to(device, dtype=torch.float32)\n",
        "                target = target.to(device, dtype=torch.int64)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = self.forward(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # Calculate Accuracy\n",
        "            accuracy = (output.argmax(dim=1) == target).float().mean()\n",
        "            # update training loss and accuracy\n",
        "            epoch_loss += loss\n",
        "            epoch_accuracy += accuracy\n",
        "\n",
        "            # perform a single optimization step (parameter update)\n",
        "            if device.type == \"xla\":\n",
        "                xm.optimizer_step(optimizer)\n",
        "\n",
        "                if i % 20 == 0:\n",
        "                    xm.master_print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n",
        "\n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n",
        "\n",
        "    def validate_one_epoch(self, valid_loader, criterion, device):\n",
        "        # keep track of validation loss\n",
        "        valid_loss = 0.0\n",
        "        valid_accuracy = 0.0\n",
        "\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "        self.model.eval()\n",
        "        for data, target in valid_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if device.type == \"cuda\":\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            elif device.type == \"xla\":\n",
        "                data = data.to(device, dtype=torch.float32)\n",
        "                target = target.to(device, dtype=torch.int64)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # forward pass: compute predicted outputs by passing inputs to the model\n",
        "                output = self.model(data)\n",
        "                # calculate the batch loss\n",
        "                loss = criterion(output, target)\n",
        "                # Calculate Accuracy\n",
        "                accuracy = (output.argmax(dim=1) == target).float().mean()\n",
        "                # update average validation loss and accuracy\n",
        "                valid_loss += loss\n",
        "                valid_accuracy += accuracy\n",
        "\n",
        "        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Ki3G2IwiNhDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def fit_tpu(\n",
        "    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n",
        "):\n",
        "\n",
        "    # valid_loss_min = np.Inf  # track change in validation loss\n",
        "    # Use np.inf instead of np.Inf\n",
        "    valid_loss_min = np.inf\n",
        "\n",
        "    # keeping track of losses as it happen\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_accs = []\n",
        "    valid_accs = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        gc.collect()\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
        "\n",
        "        xm.master_print(f\"{'='*50}\")\n",
        "        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n",
        "        train_loss, train_acc = model.train_one_epoch(\n",
        "            para_train_loader.per_device_loader(device), criterion, optimizer, device\n",
        "        )\n",
        "        xm.master_print(\n",
        "            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        gc.collect()\n",
        "\n",
        "        if valid_loader is not None:\n",
        "            gc.collect()\n",
        "            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
        "            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n",
        "            valid_loss, valid_acc = model.validate_one_epoch(\n",
        "                para_valid_loader.per_device_loader(device), criterion, device\n",
        "            )\n",
        "            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n",
        "            valid_losses.append(valid_loss)\n",
        "            valid_accs.append(valid_acc)\n",
        "            gc.collect()\n",
        "\n",
        "            # save model if validation loss has decreased\n",
        "            if valid_loss <= valid_loss_min and epoch != 1:\n",
        "                xm.master_print(\n",
        "                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n",
        "                        valid_loss_min, valid_loss\n",
        "                    )\n",
        "                )\n",
        "            #                 xm.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "    return {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"valid_acc\": valid_accs,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "VMQtUJJLNhDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = ViTBase16(n_classes=5, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlLvBaMRNhDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def _run():\n",
        "    \"\"\"\n",
        "    主训练函数\n",
        "    - 创建数据加载器\n",
        "    - 设置损失函数和优化器\n",
        "    - 执行训练循环\n",
        "    - 保存模型\n",
        "    \"\"\"\n",
        "    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n",
        "    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        valid_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=valid_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = xm.xla_device()\n",
        "    model.to(device)\n",
        "\n",
        "    lr = LR * xm.xrt_world_size()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n",
        "    start_time = datetime.now()\n",
        "    xm.master_print(f\"Start Time: {start_time}\")\n",
        "\n",
        "    logs = fit_tpu(\n",
        "        model=model,\n",
        "        epochs=N_EPOCHS,\n",
        "        device=device,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "    )\n",
        "\n",
        "    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n",
        "\n",
        "    xm.master_print(\"Saving Model\")\n",
        "    xm.save(\n",
        "        model.state_dict(), f'model_5e_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-e97ZhpNhDn",
        "outputId": "2c106e1b-ff35-40e1-c3db-426bc9927de2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIALIZING TRAINING ON 1 TPU CORES\n",
            "Start Time: 2025-05-12 07:28:44.232182\n",
            "==================================================\n",
            "EPOCH 1 - TRAINING...\n",
            "\tBATCH 1/12 - LOSS: 1.9375\n",
            "\n",
            "\t[TRAIN] EPOCH 1 - LOSS: 1.8828125, ACCURACY: 0.140625\n",
            "\n",
            "EPOCH 1 - VALIDATING...\n",
            "EPOCH 1 - VALIDATING...\t[VALID] LOSS: 1.734375, ACCURACY: 0.1875\n",
            "\n",
            "==================================================\n",
            "EPOCH 2 - TRAINING...\n",
            "==================================================\n",
            "EPOCH 2 - TRAINING...\n",
            "\tBATCH 1/12 - LOSS: 1.5625\n",
            "\n",
            "\t[TRAIN] EPOCH 2 - LOSS: 1.625, ACCURACY: 0.265625\n",
            "\n",
            "EPOCH 2 - VALIDATING...\n",
            "\t[VALID] LOSS: 1.5390625, ACCURACY: 0.375\n",
            "\n",
            "Validation loss decreased (1.7344 --> 1.5391).  Saving model ...\n",
            "==================================================\n",
            "EPOCH 3 - TRAINING...\n",
            "==================================================\n",
            "EPOCH 3 - TRAINING...\n",
            "\tBATCH 1/12 - LOSS: 1.375\n",
            "\n",
            "\t[TRAIN] EPOCH 3 - LOSS: 1.4609375, ACCURACY: 0.443359375\n",
            "\n",
            "EPOCH 3 - VALIDATING...\n",
            "EPOCH 3 - VALIDATING...\n",
            "\t[VALID] LOSS: 1.3984375, ACCURACY: 0.5\n",
            "\n",
            "Validation loss decreased (1.5391 --> 1.3984).  Saving model ...\n",
            "==================================================\n",
            "EPOCH 4 - TRAINING...\n",
            "==================================================\n",
            "EPOCH 4 - TRAINING...\n",
            "\tBATCH 1/12 - LOSS: 1.2578125\n",
            "\n",
            "\t[TRAIN] EPOCH 4 - LOSS: 1.328125, ACCURACY: 0.50390625\n",
            "\n",
            "EPOCH 4 - VALIDATING...\n",
            "\t[VALID] LOSS: 1.3125, ACCURACY: 0.5\n",
            "\n",
            "Validation loss decreased (1.3984 --> 1.3125).  Saving model ...\n",
            "==================================================\n",
            "EPOCH 5 - TRAINING...\n",
            "\n",
            "EPOCH 5 - TRAINING...\n",
            "\tBATCH 1/12 - LOSS: 1.2890625\n",
            "\n",
            "\t[TRAIN] EPOCH 5 - LOSS: 1.265625, ACCURACY: 0.58203125\n",
            "\n",
            "EPOCH 5 - VALIDATING...\n",
            "\t[VALID] LOSS: 1.2578125, ACCURACY: 0.5\n",
            "\n",
            "Validation loss decreased (1.3125 --> 1.2578).  Saving model ...\n",
            "Execution time: 0:05:59.672780\n",
            "Saving Model\n"
          ]
        }
      ],
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "    # a = _run()\n",
        "\n",
        "    # Make sure the model is created inside each process\n",
        "    # This ensures each process has its own copy of the model on the correct device\n",
        "    # global model  # Access the global model variable\n",
        "    # model = ViTBase16(n_classes=5, pretrained=True)\n",
        "    # device = xm.xla_device()\n",
        "    # model.to(device)\n",
        "    a = _run()\n",
        "\n",
        "\n",
        "# _run()\n",
        "FLAGS = {}\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method=\"fork\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9zSP4rxNhDn"
      },
      "source": [
        "## Thanks a lot for reading all the way\n",
        "\n",
        "# <font size=4 color='blue'>If you find this notebook useful, leave an upvote, that motivates me to write more such notebooks.</font>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "043b305d17354333a0275f60256e248e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "08a4eb4d2ba44906a3f6798eb4d06427": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_12e1efa0735a4eb486bf39671b3e6d17",
            "placeholder": "​",
            "style": "IPY_MODEL_75731b7033a84348af28206a7edaf5c7",
            "value": ""
          }
        },
        "0c7b124859c44d84a2269d818e4aadaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12e1efa0735a4eb486bf39671b3e6d17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223046e86c9b493789c43d5ea53c12f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489082009c2d4a93b494bc8451bd0377": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e2f573cbe1b433ab402b2017c082661",
              "IPY_MODEL_9adfced271554de5aeaae54d220e61fc",
              "IPY_MODEL_08a4eb4d2ba44906a3f6798eb4d06427",
              "IPY_MODEL_7f699a0d7e0d4c05b057975273753817",
              "IPY_MODEL_b24a0c17b0ae4e53a7d5701453a2a79f"
            ],
            "layout": "IPY_MODEL_043b305d17354333a0275f60256e248e"
          }
        },
        "5e2f573cbe1b433ab402b2017c082661": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89518f8b77ca4522bd56412dafa647a6",
            "placeholder": "​",
            "style": "IPY_MODEL_e72778867f674f199074433bb05ef833",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "66bda0755f7b49f8ab97a1c0683cd8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75731b7033a84348af28206a7edaf5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f699a0d7e0d4c05b057975273753817": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_66bda0755f7b49f8ab97a1c0683cd8f7",
            "style": "IPY_MODEL_f0e2f48b59df4745a40c8cb92c43517e",
            "tooltip": ""
          }
        },
        "89518f8b77ca4522bd56412dafa647a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968e3e28fd8844ac8d3523358dd9fcec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9adfced271554de5aeaae54d220e61fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_223046e86c9b493789c43d5ea53c12f3",
            "placeholder": "​",
            "style": "IPY_MODEL_bfe656d2615a49a8b52187b1dab7d716",
            "value": ""
          }
        },
        "b24a0c17b0ae4e53a7d5701453a2a79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_968e3e28fd8844ac8d3523358dd9fcec",
            "placeholder": "​",
            "style": "IPY_MODEL_0c7b124859c44d84a2269d818e4aadaf",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "bfe656d2615a49a8b52187b1dab7d716": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e72778867f674f199074433bb05ef833": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0e2f48b59df4745a40c8cb92c43517e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
